{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./hDAT_pIC50.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 3.264 s\n",
      "TIMING: dataset construction took 3.797 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import deepchem as dc\n",
    "from deepchem.models.tensorgraph.models.graph_models import GraphConvTensorGraph\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.feat.graph_features import ConvMolFeaturizer, WeaveFeaturizer\n",
    "\n",
    "input_data='./hDAT_pIC50.csv'\n",
    "\n",
    "tasks = ['affinity']\n",
    "featurizer=ConvMolFeaturizer()\n",
    "#featurizer=WeaveFeaturizer()\n",
    "\n",
    "loader = dc.data.CSVLoader(tasks=tasks, smiles_field=\"canonical_smiles\",featurizer=featurizer)\n",
    "dataset=loader.featurize(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 0.578 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.265 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.261 s\n",
      "Loading dataset from disk.\n",
      "<deepchem.data.datasets.DiskDataset object at 0x7fb930831160>\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import tempfile, shutil\n",
    "from deepchem.splits.splitters import RandomSplitter\n",
    "\n",
    "splitter=RandomSplitter()\n",
    "train_data,valid_data,test_data=splitter.train_valid_test_split(dataset)\n",
    "\n",
    "\n",
    "\n",
    "len(train_data),len(valid_data),len(test_data)\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nagayasu/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n",
      "Starting epoch 6\n",
      "Starting epoch 7\n",
      "Starting epoch 8\n",
      "Starting epoch 9\n",
      "Starting epoch 10\n",
      "Starting epoch 11\n",
      "Starting epoch 12\n",
      "Starting epoch 13\n",
      "Starting epoch 14\n",
      "Starting epoch 15\n",
      "Starting epoch 16\n",
      "Starting epoch 17\n",
      "Starting epoch 18\n",
      "Starting epoch 19\n",
      "Starting epoch 20\n",
      "Starting epoch 21\n",
      "Starting epoch 22\n",
      "Starting epoch 23\n",
      "Starting epoch 24\n",
      "Starting epoch 25\n",
      "Starting epoch 26\n",
      "Starting epoch 27\n",
      "Starting epoch 28\n",
      "Starting epoch 29\n",
      "Starting epoch 30\n",
      "Starting epoch 31\n",
      "Starting epoch 32\n",
      "Starting epoch 33\n",
      "Starting epoch 34\n",
      "Starting epoch 35\n",
      "Starting epoch 36\n",
      "Starting epoch 37\n",
      "Starting epoch 38\n",
      "Starting epoch 39\n",
      "Starting epoch 40\n",
      "Starting epoch 41\n",
      "Starting epoch 42\n",
      "Starting epoch 43\n",
      "Starting epoch 44\n",
      "Starting epoch 45\n",
      "Starting epoch 46\n",
      "Starting epoch 47\n",
      "Starting epoch 48\n",
      "Starting epoch 49\n",
      "Starting epoch 50\n",
      "Starting epoch 51\n",
      "Starting epoch 52\n",
      "Starting epoch 53\n",
      "Starting epoch 54\n",
      "Starting epoch 55\n",
      "Starting epoch 56\n",
      "Starting epoch 57\n",
      "Starting epoch 58\n",
      "Starting epoch 59\n",
      "Starting epoch 60\n",
      "Starting epoch 61\n",
      "Starting epoch 62\n",
      "Starting epoch 63\n",
      "Starting epoch 64\n",
      "Starting epoch 65\n",
      "Starting epoch 66\n",
      "Starting epoch 67\n",
      "Starting epoch 68\n",
      "Starting epoch 69\n",
      "Starting epoch 70\n",
      "Starting epoch 71\n",
      "Starting epoch 72\n",
      "Starting epoch 73\n",
      "Starting epoch 74\n",
      "Starting epoch 75\n",
      "Starting epoch 76\n",
      "Starting epoch 77\n",
      "Starting epoch 78\n",
      "Starting epoch 79\n",
      "Starting epoch 80\n",
      "Starting epoch 81\n",
      "Starting epoch 82\n",
      "Starting epoch 83\n",
      "Starting epoch 84\n",
      "Starting epoch 85\n",
      "Starting epoch 86\n",
      "Starting epoch 87\n",
      "Starting epoch 88\n",
      "Starting epoch 89\n",
      "Starting epoch 90\n",
      "Starting epoch 91\n",
      "Starting epoch 92\n",
      "Starting epoch 93\n",
      "Starting epoch 94\n",
      "Starting epoch 95\n",
      "Starting epoch 96\n",
      "Starting epoch 97\n",
      "Starting epoch 98\n",
      "Starting epoch 99\n",
      "Evaluating model\n",
      "computed_metrics: [0.5581185966598492]\n",
      "computed_metrics: [0.6998189145376442]\n"
     ]
    }
   ],
   "source": [
    "from deepchem.models.tensorgraph.models.graph_models import GraphConvTensorGraph, WeaveTensorGraph\n",
    "\n",
    "model = GraphConvTensorGraph(len(tasks), dropout=0.5, batch_size=50, mode='regression',tensorboard=False, learning_rate=0.0001)\n",
    "#model = WeaveTensorGraph(len(tasks), dropout=0.5, batch_size=50, mode='regression')\n",
    "\n",
    "\n",
    "model.fit(train_data, nb_epoch=100)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from deepchem.metrics import rms_score\n",
    "import deepchem.hyper.grid_search\n",
    "\n",
    "metric = dc.metrics.Metric(\n",
    "    mean_absolute_error, task_averager=np.mean, mode=\"regression\")\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_scores = model.evaluate(train_data, [metric])\n",
    "\n",
    "valid_scores = model.evaluate(valid_data, [metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.42829117]\n",
      " [6.32790214]\n",
      " [6.42365865]\n",
      " ...\n",
      " [6.        ]\n",
      " [6.        ]\n",
      " [6.        ]]\n",
      "[[6.9511333]\n",
      " [6.7873783]\n",
      " [6.8499126]\n",
      " ...\n",
      " [6.0283217]\n",
      " [6.041586 ]\n",
      " [5.9235425]]\n",
      "[[[6.9674335]]\n",
      "\n",
      " [[6.8575206]]\n",
      "\n",
      " [[7.0048122]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.3325133]]\n",
      "\n",
      " [[6.3665137]]\n",
      "\n",
      " [[5.7774878]]]\n",
      "[[[6.820111 ]]\n",
      "\n",
      " [[7.1223965]]\n",
      "\n",
      " [[7.3354883]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.3311315]]\n",
      "\n",
      " [[6.131397 ]]\n",
      "\n",
      " [[6.1499295]]]\n",
      "[[[7.2837543]]\n",
      "\n",
      " [[6.895457 ]]\n",
      "\n",
      " [[7.138221 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.0067263]]\n",
      "\n",
      " [[6.147125 ]]\n",
      "\n",
      " [[6.0368137]]]\n",
      "[[[7.2355185]]\n",
      "\n",
      " [[6.9923234]]\n",
      "\n",
      " [[7.303121 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.4236326]]\n",
      "\n",
      " [[6.236051 ]]\n",
      "\n",
      " [[5.9858074]]]\n",
      "[[[7.1506586]]\n",
      "\n",
      " [[7.1892385]]\n",
      "\n",
      " [[6.9814653]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.534867 ]]\n",
      "\n",
      " [[6.3146443]]\n",
      "\n",
      " [[6.140617 ]]]\n",
      "[[[7.10222  ]]\n",
      "\n",
      " [[7.0707793]]\n",
      "\n",
      " [[7.0819397]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.019953 ]]\n",
      "\n",
      " [[6.2231   ]]\n",
      "\n",
      " [[6.4070916]]]\n",
      "[[[7.046963 ]]\n",
      "\n",
      " [[6.9765615]]\n",
      "\n",
      " [[6.922988 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.2236276]]\n",
      "\n",
      " [[6.062258 ]]\n",
      "\n",
      " [[5.963997 ]]]\n",
      "[[[6.645162 ]]\n",
      "\n",
      " [[6.9052167]]\n",
      "\n",
      " [[6.952717 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.20708  ]]\n",
      "\n",
      " [[6.4788737]]\n",
      "\n",
      " [[6.1343203]]]\n",
      "[[[6.7253504]]\n",
      "\n",
      " [[7.0007415]]\n",
      "\n",
      " [[7.063466 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.1639338]]\n",
      "\n",
      " [[6.308664 ]]\n",
      "\n",
      " [[6.0689836]]]\n",
      "[[[6.7987003]]\n",
      "\n",
      " [[7.2058887]]\n",
      "\n",
      " [[6.9954824]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.0155663]]\n",
      "\n",
      " [[6.338473 ]]\n",
      "\n",
      " [[5.7406745]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.y)\n",
    "print(model.predict(train_data))\n",
    "\n",
    "for i in range(10):\n",
    "    print(model.predict_proba(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a70ac58861d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_y_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tg' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "def reshape_y_pred(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    TensorGraph.Predict returns a list of arrays, one for each output\n",
    "    We also have to remove the padding on the last batch\n",
    "    Metrics taks results of shape (samples, n_task, prob_of_class)\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    retval = np.stack(y_pred, axis=1)\n",
    "    return retval[:n_samples]\n",
    "\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_predictions = tg.predict_on_generator(data_generator(train_dataset, predict=True))\n",
    "train_predictions = reshape_y_pred(train_dataset.y, train_predictions)\n",
    "train_scores = metric.compute_metric(train_dataset.y, train_predictions, train_dataset.w)\n",
    "print(\"Training ROC-AUC Score: %f\" % train_scores)\n",
    "\n",
    "valid_predictions = tg.predict_on_generator(data_generator(valid_dataset, predict=True))\n",
    "valid_predictions = reshape_y_pred(valid_dataset.y, valid_predictions)\n",
    "valid_scores = metric.compute_metric(valid_dataset.y, valid_predictions, valid_dataset.w)\n",
    "print(\"Valid ROC-AUC Score: %f\" % valid_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
